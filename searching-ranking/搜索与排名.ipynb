{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 创建爬虫程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import parse\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "ignore_words = set(['the', 'of', 'to', 'and', 'a', 'in', 'is', 'it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    def __init__(self, db_name):\n",
    "        self.db = sqlite3.connect(db_name)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.db.close()\n",
    "\n",
    "    # 只有在commit之后，修改操作才会正式生效\n",
    "    def db_commit(self):\n",
    "        self.db.commit()\n",
    "\n",
    "    # 在table中查找field中为values的项，有则返回rowid，没有则插入新项\n",
    "    def get_entry_id(self, table, field, values, create=True):\n",
    "        cur = self.db.execute('''SELECT rowid\n",
    "                                FROM %s\n",
    "                                WHERE %s='%s' ''' % (table, field, values))\n",
    "        res = cur.fetchone()\n",
    "        if res is None:\n",
    "            cur = self.db.execute('''INSERT INTO %s(%s) VALUES ('%s')''' % (table, field, values))\n",
    "            return cur.lastrowid\n",
    "        else:\n",
    "            return res[0]\n",
    "\n",
    "    # 建立网页和单词之间的索引\n",
    "    def add_to_index(self, url, soup):\n",
    "        if self.is_indexed(url): return\n",
    "        print('Indexing %s .' % url)\n",
    "\n",
    "        text = self.get_text(soup)\n",
    "        words = self.split_text(text)\n",
    "\n",
    "        url_id = self.get_entry_id('url_list', 'url', url)\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            word = words[i]\n",
    "            if word in ignore_words: continue\n",
    "            word_id = self.get_entry_id('word_list', 'word', word)\n",
    "            # 网页id-单词id-单词在网页中的位置\n",
    "            self.db.execute(\n",
    "                '''INSERT INTO word_location(url_id, word_id, location) values (%d, %d, %d)''' % (url_id, word_id, i))\n",
    "\n",
    "    # 获取soup的纯文本（无html标签）\n",
    "    def get_text(self, soup):\n",
    "        # soup只有一个标签时，可以用string获取字符串；有多个标签时返回None\n",
    "        v = soup.string\n",
    "        if v is None:\n",
    "            # soup有多个标签\n",
    "            c = soup.contents\n",
    "            result_text = ''\n",
    "            for t in c:\n",
    "                # 递归提取纯文本\n",
    "                sub_text = self.get_text(t)\n",
    "                result_text += sub_text + '\\n'\n",
    "            return result_text\n",
    "        else:\n",
    "            return v.strip()\n",
    "\n",
    "    # 将纯文本按照非字母划分\n",
    "    def split_text(self, text):\n",
    "        splitter = re.compile('\\\\W*')  # ??? # ps:词干提取：porter stemmer\n",
    "        return [s.lower() for s in splitter.split(text) if s != '']\n",
    "\n",
    "    # 链接是否已经被索引，且与url与单词的关系被记录\n",
    "    def is_indexed(self, url):\n",
    "        u = self.db.execute('''SELECT rowid\n",
    "                            FROM url_list\n",
    "                            WHERE url='%s' ''' % url).fetchone()\n",
    "        # 检查和单词的关系\n",
    "        if u is not None:\n",
    "            v = self.db.execute('''SELECT *\n",
    "                                FROM word_location\n",
    "                                WHERE url_id=%d''' % u[0]).fetchone()\n",
    "            if v is not None: return True\n",
    "        return False\n",
    "\n",
    "    # 链接两个url\n",
    "    def add_link(self, url1, url2, link_text):\n",
    "        words = self.split_text(link_text)\n",
    "        id1 = self.get_entry_id('url_list', 'url', url1)\n",
    "        id2 = self.get_entry_id('url_list', 'url', url2)\n",
    "        if id1 == id2: return\n",
    "        cur = self.db.execute('''INSERT INTO link(from_id, to_id) VALUES(%d,%d)''' % (id1, id2))\n",
    "        link_id = cur.lastrowid\n",
    "        for word in words:\n",
    "            if word in ignore_words: continue\n",
    "            word_id = self.get_entry_id('word_list', 'word', word)\n",
    "            self.db.execute('''INSERT INTO link_words(link_id, word_id) VALUES(%d, %d)''' % (link_id, word_id))\n",
    "\n",
    "    # 爬取网页\n",
    "    def crawl(self, links, depth=2):\n",
    "        for i in range(depth):\n",
    "            new_links = set()\n",
    "            for link in links:\n",
    "                try:\n",
    "                    req = requests.get(link)\n",
    "                except:\n",
    "                    # get失败\n",
    "                    print('Could not get %s .' % link)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    soup = BeautifulSoup(req.text)\n",
    "                    # 当前链接建立索引\n",
    "                    self.add_to_index(link, soup)\n",
    "                    self.db_commit()\n",
    "\n",
    "                    c_links = soup('a')  # list\n",
    "                    for c_link in c_links:  # soup\n",
    "                        if 'href' in c_link.attrs:  # dict\n",
    "                            url = parse.urljoin(link, c_link['href'])  # 智能连接\n",
    "\n",
    "                            if url.find(\"'\") != -1:  # 有一些特殊的网址很奇怪有引号\n",
    "                                continue\n",
    "                            url = url.split('#')[0]  # 去除位置标记\n",
    "                            # 加入新链接\n",
    "                            if url[:4] == 'http' and not self.is_indexed(url):\n",
    "                                new_links.add(url)\n",
    "                            # 获取url描述文本，连接两个url\n",
    "                            c_link_text = self.get_text(c_link)\n",
    "                            self.add_link(link, url, c_link_text)\n",
    "                            self.db_commit()\n",
    "                except:\n",
    "                    # soup转化失败\n",
    "                    print('Could not parse page %s .' % link)\n",
    "\n",
    "            links = new_links\n",
    "\n",
    "    # 初始化数据库\n",
    "    def create_index_table(self):\n",
    "        # 创建5各个表\n",
    "        self.db.execute('''CREATE TABLE url_list\n",
    "                            (url TEXT)''')\n",
    "\n",
    "        self.db.execute('''CREATE TABLE word_list\n",
    "                            (word TEXT)''')\n",
    "\n",
    "        self.db.execute('''CREATE TABLE word_location\n",
    "                            (url_id INTEGER,\n",
    "                            word_id INTEGER,\n",
    "                            location INTEGER)''')\n",
    "\n",
    "        # 链接连接关系表\n",
    "        self.db.execute('''CREATE TABLE link\n",
    "                            (from_id INTEGER,\n",
    "                            to_id INTEGER)''')\n",
    "        # 链接连接边信息表：链接描述\n",
    "        self.db.execute('''CREATE TABLE link_words\n",
    "                            (word_id INTEGER,\n",
    "                            link_id INTEGER)''')\n",
    "        # 索引可以加快查询速度，但是会减慢插入速度\n",
    "        self.db.execute('CREATE INDEX word_idx ON word_list(word)')\n",
    "        self.db.execute('CREATE INDEX url_idx ON url_list(url)')\n",
    "        self.db.execute('CREATE INDEX word_url_idx ON word_location(word_id)')\n",
    "        self.db.execute('CREATE INDEX url_to_idx ON link(to_id)')\n",
    "        self.db.execute('CREATE INDEX url_from_idx ON link(from_id)')\n",
    "\n",
    "        self.db_commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spi = crawler('searchindex.db')\n",
    "spi.create_index_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:61: FutureWarning: split() requires a non-empty pattern match.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing https://www.baidu.com .\n",
      "Indexing http://ir.baidu.com .\n",
      "Indexing http://www.baidu.com/duty/ .\n",
      "Indexing http://tieba.baidu.com .\n",
      "Indexing http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 .\n",
      "Indexing http://jianyi.baidu.com/ .\n",
      "Indexing http://v.baidu.com .\n",
      "Indexing http://news.baidu.com .\n",
      "Indexing http://map.baidu.com .\n",
      "Indexing https://www.hao123.com .\n",
      "Indexing http://home.baidu.com .\n",
      "Indexing https://www.baidu.com/more/ .\n"
     ]
    }
   ],
   "source": [
    "lst = ['https://www.baidu.com']\n",
    "spi.crawl(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0), (10, 0), (11, 0), (11, 4), (6, 0), (6, 3212), (6, 5971), (7, 0), (7, 109), (7, 111), (12, 0), (12, 8), (12, 47), (12, 49), (5, 0), (5, 1771), (5, 2194), (5, 2196), (2, 0), (2, 2897), (2, 3438), (2, 3468), (2, 3493), (2, 3523), (2, 3548), (2, 3578), (2, 3652), (2, 3682), (4, 0), (4, 420), (4, 434), (4, 478), (4, 492), (3, 0), (3, 260), (3, 22561), (3, 23298), (3, 23310), (3, 24543), (3, 24562), (3, 24580), (3, 24599), (3, 29205), (3, 29406), (3, 29916), (3, 30080), (3, 30243), (3, 30830), (3, 31024), (3, 31181), (3, 31321), (3, 31486), (3, 31637), (3, 32112), (3, 32487), (3, 32566), (3, 32643), (3, 32725), (3, 32857), (3, 32884), (3, 32936), (3, 32963), (3, 32996), (3, 33049), (3, 33103), (3, 33338), (3, 33387), (3, 33431), (3, 35197), (3, 35425), (3, 35556), (3, 35586), (3, 35616), (3, 35650), (3, 35684), (3, 35716), (3, 35749), (3, 35782), (3, 35813), (3, 35844), (3, 35875), (3, 35907), (3, 35938), (3, 35970), (3, 36000), (3, 36032), (3, 36062), (3, 36094), (3, 36124), (3, 36154), (3, 37194), (3, 37214), (3, 37234), (3, 37253), (3, 37273), (3, 37294), (3, 37337), (3, 37359), (3, 37380), (3, 39370), (3, 39401), (3, 41453), (3, 41550), (3, 41554), (3, 44805), (3, 50670), (3, 50952), (3, 50966), (3, 51059), (3, 51447), (3, 51791), (3, 51972), (3, 52146), (3, 52372), (3, 52532), (3, 52704), (3, 52881), (3, 53057), (3, 53226), (3, 53440), (3, 53607), (3, 54324), (3, 54373), (3, 54388), (3, 54856), (3, 54910), (3, 54964), (3, 55018), (3, 55072), (3, 55126), (3, 55232), (3, 55289), (3, 55301), (3, 55358), (3, 55370), (3, 55427), (3, 55439), (3, 55496), (3, 55508), (3, 55565), (3, 55577), (3, 55634), (3, 55646), (3, 55703), (3, 55715), (3, 55772), (3, 55784), (3, 55846), (3, 55858), (3, 55918), (3, 55930), (3, 55990), (3, 56002), (3, 56062), (3, 56074), (3, 56134), (3, 56146), (3, 56206), (3, 56218), (3, 56278), (3, 56290), (3, 56350), (3, 56362), (3, 56422), (3, 56434), (3, 56496), (3, 56508), (3, 56568), (3, 56580), (3, 56640), (3, 56652), (3, 56712), (3, 56724), (3, 56784), (3, 56796), (3, 56856), (3, 56868), (3, 56928), (3, 57017), (3, 57208), (3, 57268), (3, 57280), (3, 57340), (3, 57352), (3, 57412), (3, 57424), (3, 57484), (3, 57496), (3, 57556), (3, 57568), (3, 57628), (3, 57640), (3, 57700), (3, 57712), (3, 57772), (3, 57784), (3, 57846), (3, 57858), (3, 57918), (3, 57930), (3, 57990), (3, 58002), (3, 58062), (3, 58074), (3, 58134), (3, 58146), (3, 58206), (3, 58218), (3, 58278), (3, 58290), (3, 58350), (3, 58362), (3, 58422), (3, 58434), (3, 58493), (3, 58505), (3, 58565), (3, 58577), (3, 58634), (3, 58646), (3, 58703), (3, 58715), (3, 58772), (3, 58784), (3, 58841), (3, 58853), (3, 58910), (3, 59078), (3, 59520), (3, 59750), (3, 60084), (3, 60254), (3, 60343), (3, 60811), (3, 60865), (3, 60918), (3, 61262), (3, 61319), (3, 61389), (3, 61721), (3, 61774), (3, 61828), (3, 62425), (3, 62481), (3, 62552), (3, 62658), (3, 62712), (3, 62768), (3, 63406), (3, 63535), (3, 63569), (3, 63603), (3, 63637), (3, 63671), (3, 63705), (3, 63739), (3, 63773), (3, 63807), (3, 63841), (3, 63888), (3, 63925), (3, 63959), (3, 63993), (3, 64027), (3, 64061), (3, 64095), (3, 64129), (3, 64163), (3, 64197), (3, 65040), (3, 65835), (3, 66144), (3, 66767), (3, 66892), (3, 67017), (3, 67287), (9, 0), (9, 490), (9, 511), (8, 0), (8, 3936)]\n"
     ]
    }
   ],
   "source": [
    "# id为1的单词的（url, location）\n",
    "out = [row for row in spi.db.execute('''SELECT url_id, location\n",
    "                                        FROM word_location\n",
    "                                        WHERE word_id=1''')]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.NavigableString'> ['æ\\x96°é\\x97»']\n"
     ]
    }
   ],
   "source": [
    "print(type(z.contents[0]), z.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
